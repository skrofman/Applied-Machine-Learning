{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA_310_Lecture_21_Spring_2021.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skrofman/Applied-Machine-Learning/blob/master/DATA_310_Lecture_21_Spring_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcRP_HopYg5b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error as MAE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "import xgboost as xgb "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orUuSuPlZYEs"
      },
      "source": [
        "dat = load_boston()\n",
        "df = pd.DataFrame(data=dat.data,columns=dat.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsQfJ8KSZyvh"
      },
      "source": [
        "X = dat.data\n",
        "y = dat.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H0X_-MIg4_B",
        "outputId": "a819a815-5bf0-4d20-b1c2-97bedbdb3071"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEkq9QweaCCJ"
      },
      "source": [
        "## Definition: Root Mean Squared Error\n",
        "\n",
        "$$\\mbox{RMSE}:=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{p}(y_i-\\hat{y_i})^2}$$\n",
        "\n",
        "(in \"loose\" terms this is sometimes called the empirical norm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lgVMLsncOHs"
      },
      "source": [
        "def RMSE(y,yhat):\n",
        "  return np.sqrt(MSE(y,yhat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuBjbwSZd1Rc"
      },
      "source": [
        "def MAEf(y,yhat):\n",
        "  n=len(y)\n",
        "  return np.sum(np.abs(y-yhat))/n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABjLWaYPZzVJ"
      },
      "source": [
        "def DoKFold(X,y,model,k):\n",
        "  PE1 = []\n",
        "  PE2 = []\n",
        "  kf = KFold(n_splits=k,shuffle=True,random_state=1234)\n",
        "  for idxtrain,idxtest in kf.split(X):\n",
        "    Xtrain = X[idxtrain,:]\n",
        "    Xtest  = X[idxtest,:]\n",
        "    ytrain = y[idxtrain]\n",
        "    ytest  = y[idxtest]\n",
        "    model.fit(Xtrain,ytrain)\n",
        "    yhat = model.predict(Xtest)\n",
        "    PE1.append(MAEf(ytest,yhat))\n",
        "    PE2.append(RMSE(ytest,yhat))\n",
        "  return 1000*np.mean(PE1), 1000*np.mean(PE2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np9pwC74bVGX"
      },
      "source": [
        "### Querstion: Do we need scaling for Random Forest?\n",
        " Answer: No."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_1dFlvDbbcU"
      },
      "source": [
        "model = RandomForestRegressor(n_estimators=500,max_depth=10,random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSq-r-Hpc-gm",
        "outputId": "3c272e45-c2c2-4f14-d0c8-60dc3352075a"
      },
      "source": [
        "DoKFold(X,y,model,10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2273.5560051999537, 3394.701250044423)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3srB8kwdEEv"
      },
      "source": [
        "model = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=500,reg_lambda=1,alpha=1,gamma=1,max_depth=10,random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3wisXZWe41w",
        "outputId": "1c9050b1-80ce-4ae6-f9a2-6e44478aecd4"
      },
      "source": [
        "DoKFold(X,y,model,10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2092.731559730978, 3176.011342160705)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdoHsUNLe6gw"
      },
      "source": [
        "# imports for creating a Neural Networks (Deep Learning)\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import r2_score\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyiWuwKJgZFp"
      },
      "source": [
        "# Create a Neural Network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation=\"relu\", input_dim=13))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(8, activation=\"relu\"))\n",
        "# Since the regression is performed, a Dense layer containing a single neuron with a linear activation function.\n",
        "# Typically ReLu-based activation are used but since it is performed regression, it is needed a linear activation.\n",
        "model.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "# Compile model: The model is initialized with the Adam optimizer and then it is compiled.\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-3, decay=1e-3 / 200))\n",
        "\n",
        "# Patient early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSF2IpRxghn9",
        "outputId": "e87e4547-5d34-4b26-a58f-93268abedf94"
      },
      "source": [
        "mae_nn = []\n",
        "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
        "for idxtrain, idxtest in kf.split(X):\n",
        "  X_train = X[idxtrain,:]\n",
        "  y_train = y[idxtrain]\n",
        "  X_test  = X[idxtest,:]\n",
        "  y_test = y[idxtest]\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "  model.fit(X_train,y_train,validation_split=0.3, epochs=1000, batch_size=100, verbose=0, callbacks=[es])\n",
        "  yhat_nn = model.predict(X_test)\n",
        "  mae_nn.append(MAE(y_test, yhat_nn))\n",
        "print(\"Validated MAE Neural Network Regression = ${:,.2f}\".format(1000*np.mean(mae_nn)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00466: early stopping\n",
            "Epoch 00254: early stopping\n",
            "Epoch 00272: early stopping\n",
            "Epoch 00206: early stopping\n",
            "Epoch 00246: early stopping\n",
            "Epoch 00204: early stopping\n",
            "Epoch 00393: early stopping\n",
            "Epoch 00296: early stopping\n",
            "Epoch 00638: early stopping\n",
            "Validated MAE Neural Network Regression = $4,127.29\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}