{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA_310_Lecture_38_Spring_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skrofman/Applied-Machine-Learning/blob/master/DATA_310_Lecture_38_Spring_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsv8GUYM6qR1"
      },
      "source": [
        "# DATA 310 - Lecture 38"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47zhKygt16DR"
      },
      "source": [
        "##<font color='darkgreen' face='Papyrus' size=12pt>Natural Language Processing</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeSccFn3kPfE"
      },
      "source": [
        "<font color='crimson'>**Objective:** use speech and words along with computer run algorithms.\n",
        "\n",
        "<span style=\"font-family:Calibri; color:darkblue; font-size:18pt;\">Examples of projects/research with NLP:</span>\n",
        "\n",
        "<font color='blue'>*Sentiment Analysis*</font> - How positive or negative is text about a topic? \n",
        "\n",
        "<font color='blue'>*Prediction*</font> - What genres should Netflix classify a movie as to maximize views? Based on product reviews, can we predict the star rating of a product?\n",
        "\n",
        "<font color='blue'>*Translation*</font> - Recognize words in one language to provide similar words in another.\n",
        "\n",
        "**Playground:** https://www.deepl.com/translator\n",
        "\n",
        "\n",
        "<font color='blue'>*Summarization*</font> - Take a long document and produce a shorter one (a synthesis) without losing meaningful information.\n",
        "\n",
        "\n",
        "<font color='forestgreen'>**Methods:**</font> <span style=\"font-family:Calibri; color:red; font-size:12pt;\">The main idea is to quantify the occurrence of relevant words and, based on the context, to map them into vectors. That is to say that we want to create mathematically representable quantities from words and text; they will serve as features for data analysis. One approach is separate the text data into sentences and then sentences can be used to extract (key) words and expressions.</span>\n",
        "\n",
        "###**Regular Expressions (regex)**\n",
        "\n",
        "Goal: provide a language that allows us to search for different text strings.\n",
        "\n",
        "For example, Regular Expressions (frequently called “regex”) allows us to label all tweets with a “1” if they contain the following list of words:\n",
        "\n",
        "- college\n",
        "- College of\n",
        "- colleges\n",
        "- The College\n",
        "\n",
        "The idea is to detect that in all expressions above we have the same concept \"college\".\n",
        "\n",
        "\n",
        "\n",
        "<font color='blue' face='Calibri' size=5pt>Examples of common REGEX patterns</font>\n",
        "\n",
        "**[tT]**imber  - would match lower or uppercase T\n",
        "\n",
        "**[A-Z]** - would match any capital character\n",
        "\n",
        "**[a-z]** - would match any lowercase character\n",
        "\n",
        "**[0-9]** - would match any single number (i.e., 9)\n",
        "\n",
        "**[^A-Z]** - would match anything that isn’t an uppercase letter.\n",
        "\n",
        "**\\w** - would match any letter.\n",
        "\n",
        "A comprehensive manual on regex can be found here:\n",
        "https://www3.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjXxyULYjYT9"
      },
      "source": [
        "# import regex in Python\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XnTsc2O60Ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb7708f-6a90-4f2f-e617-3394c8919a5b"
      },
      "source": [
        "pattern = r\"[dD]elicious\"\n",
        "sequence = \"Chocolate is not delicious\"\n",
        "sequence2 = \"This new recipe deliciously implemented a new idea about the texture of the chocolate.\"\n",
        "if re.search(pattern, sequence2):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Match!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BaUHIou70ez"
      },
      "source": [
        "###An example for replacing the spaces between words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKtrHwsD65vM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "310d74f6-36fa-452f-e93f-a09afedca549"
      },
      "source": [
        "text = \"This chocolate is delicious but it may have too many calories, such as 400.\"\n",
        "re.sub('[^a-zA-Z0-9]','',text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Thischocolateisdeliciousbutitmayhavetoomanycaloriessuchas400'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dk4zqK08L7y"
      },
      "source": [
        "###An example for matching a patttern (a sequence of characters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL91n3Zy7x08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6103404d-9d96-4bd6-bb06-5124eb6ee8b6"
      },
      "source": [
        "pattern = r\"\\w\\w\\wcolate\"\n",
        "sequence = \"Chocolate is delicious\"\n",
        "if re.match(pattern, sequence):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Match!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAp3N5p88Wwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a8e3ff-2169-424c-b6a2-3067497da412"
      },
      "source": [
        "pattern = r\"good for you\"\n",
        "sequence = \"Chocolate is delicious and good for you\"\n",
        "if re.search(pattern, sequence):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Match!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeXHM2fn9zil"
      },
      "source": [
        "###Example:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1AMHbSgq3MHcv8Q8ljnHvl5IkxTKkzGkx' \n",
        "width='600px' />\n",
        "<figcaption>Data from Twitter</figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr1AOY-18eRD"
      },
      "source": [
        "text = \"\"\"Rep. Stephanie Murphy Verified account @RepStephMurphy Aug 30 More Celebrating 100yrs of coeducation at @williamandmary, \n",
        "        it was a true honor to return to my alma mater & join its first female president, Katherine Rowe, to welcome students at their convocation. \n",
        "        I spoke about the power of patriotism & the urgent need for active, engaged citizens.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51YBSini_hUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f649483-f052-41c3-f4f9-538e464becd0"
      },
      "source": [
        "pattern = r\"[cC]elebrating\"\n",
        "if re.search(pattern, text):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Match!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfxPZBsd_mXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238feef9-06c6-49eb-af2a-3acb5ccb6872"
      },
      "source": [
        "pattern = r\"[C|c]elebrat[a-z]+\"\n",
        "if re.search(pattern, text):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Match!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WE1A7C8AeVy"
      },
      "source": [
        "<font face='Calibri' color='blue' size=5pt>The Bag of Words model (BoW)</font>\n",
        "\n",
        "**Main Goal:** use concurrences within context and counts of keywords to make predictions.\n",
        "\n",
        "**Observation:** there are many words that do not matter (such as prepositions or definite and indefinite articles). \n",
        "\n",
        "**Important:** each word can be translated into a binary value of occurrence.\n",
        "\n",
        "<span style=\"font-family:Calibri; color:darkblue; font-size:5pt;\">Analog Example:</span>\n",
        "\n",
        "*Statement 1*: Jurassic World was the pinnacle of human achievement.\n",
        "\n",
        "*Statement 2*: Human kind would be better without Jurassic World.\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1EUGNgop58BOOhFGHR3iKs5gXbrji6jEM' \n",
        "width='600px' />\n",
        "<figcaption>What is the difference in the statements above?</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "**Method**: we discard the *stopwords* such as articles, prepositions, verbs and retain the *corpus* (important words or *roots* of important words).\n",
        "\n",
        "\n",
        "\n",
        "A simple model based on this data:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1-uuXfXiYlmub8DauhxhYYCP2TKqfdvoB' \n",
        "width='600px' />\n",
        "<figcaption>The differences can be highlighted by using a count/vectorizer method</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "**Main idea:** analyze differences and co-occurrencies.\n",
        "\n",
        "**Known Problems:**\n",
        "\n",
        " - If some sentences are much longer in length, the vocabulary would increase and as such, the length of the vectors would increase; this is a dimensionality problem.\n",
        " - The new sentences may contain more different words from the previous sentences.\n",
        " - The vectors would also contain many zeros, thereby resulting in a sparse matrix.\n",
        " - No information on the grammatical structure or the actual ordering of the words is being used.\n",
        "\n",
        "**Possible Solution:** Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "The term frequency-inverse document frequency is a measure that quantifies the importance of a word in the context of a document or a *corpus*.\n",
        "\n",
        "The *term-frequency* of a word is the relative frequency of the term in the context of the document.\n",
        "\n",
        "$$\\text{TF}(t,d):=\\frac{\\text{# of times the term appears in the document}}{\\text{# of terms in the document }}$$\n",
        "\n",
        "\n",
        "The *inverse document frequency* is defined as:\n",
        "\n",
        "$$\\text{IDF}(t,d):=\\log\\left(\\frac{\\text{# of documents}}{\\text{# of documents with term } t}\\right)$$\n",
        "\n",
        "Our quantification of relative importance is defined as the product between TF and IDF.\n",
        "\n",
        "TF-IDF gives larger values for less frequent words and is high when both IDF and TF values are high, for instance the word is rare in all the documents combined but frequent in a single document.\n",
        "\n",
        "A good Python example can be found here: \n",
        "\n",
        "https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-rVM7uOAFj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def32f4e-94cf-4be2-bec2-fadac751f4fd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcgT03h3BW-t"
      },
      "source": [
        "text = open(\"Christmas_Carol.txt\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmmUvIi4YTSy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fdcd4ec7-1a8e-4486-c8c5-c5fdaf4c8296"
      },
      "source": [
        "text[1:800]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" CHRISTMAS CAROL\\n\\nIN PROSE\\nBEING\\nA Ghost Story of Christmas\\n\\nby Charles Dickens\\n\\n\\n\\nPREFACE\\n\\nI HAVE endeavoured in this Ghostly little book,\\nto raise the Ghost of an Idea, which shall not put my\\nreaders out of humour with themselves, with each other,\\nwith the season, or with me.  May it haunt their houses\\npleasantly, and no one wish to lay it.\\n\\nTheir faithful Friend and Servant,\\n                                   C. D.\\nDecember, 1843.\\n\\n\\n\\nCONTENTS\\n\\nStave   I: Marley's Ghost\\nStave  II: The First of the Three Spirits\\nStave III: The Second of the Three Spirits\\nStave  IV: The Last of the Spirits\\nStave   V: The End of It\\n\\n\\n\\nSTAVE I:  MARLEY'S GHOST\\n\\nMARLEY was dead: to begin with. There is no doubt\\nwhatever about that. The register of his burial was\\nsigned by the clergyman, the clerk, the undert\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwPgD4RsBeVS"
      },
      "source": [
        "<font face=\"Calibri\" color='navy' size=4pt>We can extract all the sentences (based on punctuation):</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PobVsJyzB9mJ"
      },
      "source": [
        "dataset = nltk.sent_tokenize(text) \n",
        "for i in range(len(dataset)): \n",
        "    dataset[i] = dataset[i].lower() \n",
        "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
        "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIAkuuEdCEZ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f69570e4-cac8-4c8a-e565-ce2a3c9f1836"
      },
      "source": [
        "# this is the 1900th sentence\n",
        "dataset[1899]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'so did the plump sister when she came '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VbuMt_-3gNt"
      },
      "source": [
        "What do you notice? There are no capital letters, no punctuation (because the computer does not need them).\n",
        "\n",
        "We can also determine how frequent are the different words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liv9PCWGCOp7"
      },
      "source": [
        "# we can count the occurrencies of different words\n",
        "# Creating the Bag of Words model \n",
        "word2count = {} \n",
        "for data in dataset: \n",
        "    words = nltk.word_tokenize(data) \n",
        "    for word in words: \n",
        "        if word not in word2count.keys(): \n",
        "            word2count[word] = 1\n",
        "        else: \n",
        "            word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdglAkPpCZp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21e10e7-a2cd-4020-c69d-cda1f07a1d6f"
      },
      "source": [
        "word2count.get('wealth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgPnOqmDOzp"
      },
      "source": [
        "This means that the word \"ghost\" appeared 95 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icqKTzLnCaZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d86bed-b0ae-4d95-c50e-45bc826a9906"
      },
      "source": [
        "word2count.get('scrooge')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "362"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81WOQYhDQn5"
      },
      "source": [
        "<font face=\"Calibri\" color='navy' size=4pt>We can determine what are the most frequent words, for example:</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT3CP1V4DhxL"
      },
      "source": [
        "# the top 100 most frequent words\n",
        "import heapq \n",
        "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
        "freq_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldxUtPD_EIZs"
      },
      "source": [
        "Indeed this is a story about \"Scrooge\" and \"ghosts\"...\n",
        "\n",
        "Important: we want to discard all the unimportant words (as known as \"stopwords\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnDYV22LEAKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20280244-bdf3-45c2-98a4-d424047575cb"
      },
      "source": [
        "# Stopword dictionary\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# For stemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cv4uf8jEmfY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b2cbda07-c18a-4fcd-b7f4-eb97a1c41e06"
      },
      "source": [
        "txt = re.sub('[^a-zA-Z0-9 ]','',dataset[0])\n",
        "# Make everything lower case\n",
        "txt = txt.lower()\n",
        "# Make it a list of words\n",
        "txt = txt.split()\n",
        "# Get all the stop words out\n",
        "txt = [word for word in txt if not word in set(stopwords.words('english'))]\n",
        "# Stem the words\n",
        "txt = [stemmer.stem(word) for word in txt]\n",
        "# Put it all back together and look at the result\n",
        "' '.join(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'christma carol prose ghost stori christma charl dicken prefac endeavour ghostli littl book rais ghost idea shall put reader humour season'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cShN8rfsEyL5"
      },
      "source": [
        "..and we want to do this for every sentence in the book:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZkkVIhVExLH"
      },
      "source": [
        "corpus = []\n",
        "for i in range(len(dataset)):\n",
        "    txt = re.sub('[^a-zA-Z0-9 ]','',dataset[i])\n",
        "    txt = txt.lower()\n",
        "    txt = txt.split()\n",
        "    txt = [word for word in txt if not word in set(stopwords.words('english'))]\n",
        "    txt = [stemmer.stem(word) for word in txt]\n",
        "    txt = ' '.join(txt)\n",
        "    corpus.append(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n2VU3JwE4DL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611cbe96-2c4b-4c61-8f16-5805ef290969"
      },
      "source": [
        "corpus[100:120]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['clerk tank involuntarili applaud',\n",
              " 'becom immedi sensibl improprieti poke fire extinguish last frail spark ever',\n",
              " 'let hear anoth sound said scroog keep christma lose situat',\n",
              " 'quit power speaker sir ad turn nephew',\n",
              " 'wonder go parliament',\n",
              " 'angri uncl',\n",
              " 'come',\n",
              " 'dine us morrow',\n",
              " 'scroog said would see ye inde',\n",
              " 'went whole length express said would see extrem first',\n",
              " '',\n",
              " 'cri scroog nephew',\n",
              " '',\n",
              " 'get marri',\n",
              " 'said scroog',\n",
              " 'fell love',\n",
              " 'fell love',\n",
              " 'growl scroog one thing world ridicul merri christma',\n",
              " 'good afternoon',\n",
              " 'nay uncl never came see happen']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZOJeHKFA7r"
      },
      "source": [
        "# we can count the occurrencies of different words in the corpus\n",
        "# Creating the Bag of Words model \n",
        "word2count = {} \n",
        "for data in corpus: \n",
        "    words = nltk.word_tokenize(data) \n",
        "    for word in words: \n",
        "        if word not in word2count.keys(): \n",
        "            word2count[word] = 1\n",
        "        else: \n",
        "            word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbheF3ELFGyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80507504-8ed9-4f66-9d44-c6fe4fddf1be"
      },
      "source": [
        "# .. and get the top 10 most frequent in the corpus:\n",
        "freq_words = heapq.nlargest(20, word2count, key=word2count.get)\n",
        "freq_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scroog',\n",
              " 'said',\n",
              " 'upon',\n",
              " 'one',\n",
              " 'spirit',\n",
              " 'ghost',\n",
              " 'christma',\n",
              " 'would',\n",
              " 'hand',\n",
              " 'man',\n",
              " 'look',\n",
              " 'time',\n",
              " 'like',\n",
              " 'old',\n",
              " 'know',\n",
              " 'good',\n",
              " 'littl',\n",
              " 'could',\n",
              " 'cri',\n",
              " 'come']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NGVIDzFL4J"
      },
      "source": [
        "## Application to Amazon customer reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeNYrf-JcNp_"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soXF4QCQFOtz"
      },
      "source": [
        "df = pd.read_csv('7817_1.csv', quoting=2 )\n",
        "# Extract the ratings and text reviews\n",
        "data = df[['reviews.text', 'reviews.rating']].dropna().reset_index(drop=True)\n",
        "\n",
        "reviews = data['reviews.text']\n",
        "y = data['reviews.rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n92B5NCDcSRI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "497cb079-ccad-4364-d443-00bebbd23039"
      },
      "source": [
        "data.loc[2,'reviews.text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I am enjoying it so far. Great for reading. Had the original Fire since 2012. The Fire used to make my eyes hurt if I read too long. Haven't experienced that with the Paperwhite yet.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDhklObNFb-V"
      },
      "source": [
        "To learn more about the data:   \n",
        "\n",
        "https://www.kaggle.com/bittlingmayer/amazonreviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_7DersbFcfR"
      },
      "source": [
        "corpus = []\n",
        "for i in range(len(reviews)):\n",
        "    txt = re.sub('[^a-zA-Z0-9 ]','',reviews[i])\n",
        "    txt = txt.lower()\n",
        "    txt = txt.split()\n",
        "    txt = [word for word in txt if not word in set(stopwords.words('english'))]\n",
        "    txt = [stemmer.stem(word) for word in txt]\n",
        "    txt = ' '.join(txt)\n",
        "    corpus.append(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRgcjbYWIRRP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e222d634-210c-45a4-d9d7-0b5577511ad7"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'initi troubl decid paperwhit voyag review less said thing paperwhit great spend money go voyagefortun friend own end buy paperwhit basi model 300 ppi 80 dollar jump turn pricey voyag page press isnt alway sensit fine specif set dont need auto light adjustmentit week love paperwhit regret touch screen recept easi use keep light specif set regardless time day case hard chang set either youll chang light level certain time day everi readingalso glad went intern ship option amazon extra expens deliveri time track didnt need worri custom may use third parti ship servic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWb4ZixhFpQ1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X_raw = cv.fit_transform(corpus)\n",
        "X = X_raw.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXibZEqWdPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f47938-6b14-4e1a-88a5-350802fa4f66"
      },
      "source": [
        "X[0,:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukfkEpSMFuie"
      },
      "source": [
        "yb = y.where(y==5, other=0).where(y<5, other=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqSbY30WIx1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5525949-53fe-4b21-d7de-6d6fcc4419e2"
      },
      "source": [
        "yb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1.0\n",
              "1       1.0\n",
              "2       0.0\n",
              "3       1.0\n",
              "4       1.0\n",
              "       ... \n",
              "1172    0.0\n",
              "1173    0.0\n",
              "1174    0.0\n",
              "1175    0.0\n",
              "1176    0.0\n",
              "Name: reviews.rating, Length: 1177, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Do_3AbH9IG"
      },
      "source": [
        "### Logistic Regression Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSBtBGAbF9Uj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "63653f8e-e4ea-4547-be6c-7e6f493898a3"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "\n",
        "Xtrain,Xtest,ytrain,ytest = tts(X,yb,random_state=1234,test_size=0.25)\n",
        "cls = LogisticRegression(random_state=1234, solver='lbfgs')\n",
        "cls.fit(Xtrain,ytrain)\n",
        "ypred = cls.predict(Xtest)\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "pd.DataFrame(cm, columns=['Not 5', '5'], index =['Not 5', '5'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Not 5</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Not 5</th>\n",
              "      <td>56</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30</td>\n",
              "      <td>159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Not 5    5\n",
              "Not 5     56   50\n",
              "5         30  159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIO4S8lWJjdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3ecc22-50d8-4104-def1-9a55ce574973"
      },
      "source": [
        "acc(ytest,ypred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7288135593220338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNNCmCZwH_ky"
      },
      "source": [
        "### Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR17RiZMHkHa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "a2540f0f-465a-4a46-d1db-b7a188560aa5"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "cls = GaussianNB()\n",
        "cls.fit(Xtrain,ytrain)\n",
        "ypred = cls.predict(Xtest)\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "pd.DataFrame(cm, columns=['Not 5', '5'], index =['Not 5', '5'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Not 5</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Not 5</th>\n",
              "      <td>72</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>76</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Not 5    5\n",
              "Not 5     72   34\n",
              "5         76  113"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy4_0A11eZ6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c97b1c4-5b7c-4de5-ab7b-bdc53950bfea"
      },
      "source": [
        "acc(ytest,ypred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6271186440677966"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pea2T1NnIEC7"
      },
      "source": [
        "### Random Forest Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9dpA18ZF6HZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "f3af6da1-6ae3-4b6b-fbb1-6a6941f95332"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "Xtrain,Xtest,ytrain,ytest = tts(X,yb,random_state=1234,test_size=0.25)\n",
        "cls = RandomForestClassifier(random_state=1234, max_depth=100, n_estimators = 100)\n",
        "cls.fit(Xtrain,ytrain)\n",
        "ypred = cls.predict(Xtest)\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "pd.DataFrame(cm, columns=['Not 5', '5'], index =['Not 5', '5'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Not 5</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Not 5</th>\n",
              "      <td>47</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>11</td>\n",
              "      <td>178</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Not 5    5\n",
              "Not 5     47   59\n",
              "5         11  178"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va1QXBwpetaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ac3bbf-b231-4a52-f0ba-aff9f465691f"
      },
      "source": [
        "acc(ytest,ypred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7627118644067796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jGLmCiMI_sg"
      },
      "source": [
        "## Application to wine ratings based on customer reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUtoy0hQI_Io"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GckK-WM1JlgQ"
      },
      "source": [
        "%%time\n",
        "wine_data = pd.read_csv('winemagdata130kv2.csv',quoting=2)\n",
        "wines = wine_data[[\"description\",\"points\"]]\n",
        "wines_subset = wines.sample(1000,random_state=1693).reset_index(drop=True)\n",
        "corpus = []\n",
        "\n",
        "for i in range(0,len(wines_subset)):\n",
        "    wine_descriptions = re.sub('[^a-zA-Z0-9 ]','',wines_subset[\"description\"][i])\n",
        "    wine_descriptions=wine_descriptions.lower()\n",
        "    wine_descriptions = wine_descriptions.split()\n",
        "    wine_descriptions = [word for word in wine_descriptions if not word in set(stopwords.words('english'))]\n",
        "    stemmer = PorterStemmer()\n",
        "    wine_descriptions = [stemmer.stem(word) for word in wine_descriptions]\n",
        "    wine_descriptions = \" \".join(wine_descriptions)\n",
        "    corpus.append(wine_descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSlRbCJpJ2uO"
      },
      "source": [
        "%%time\n",
        "countVec = CountVectorizer()\n",
        "X_raw = countVec.fit_transform(corpus)\n",
        "X = X_raw.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-0xqoC4J_X0"
      },
      "source": [
        "#### Visualize the distribution of the wine ratings (points)\n",
        "n, bins, patches = plt.hist(wines_subset[\"points\"].values,10,density=1,facecolor='green',alpha=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ASsfjQKCro"
      },
      "source": [
        "y = wines_subset[\"points\"]\n",
        "y = y.where(y>90,other=0).where(y<=90,other=1).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlkycIzvKID2"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = tts(X,y,test_size=0.25,random_state=1693)\n",
        "#scale_X = StandardScaler()\n",
        "#X_train = scale_X.fit_transform(X_train)\n",
        "#X_test = scale_X.transform(X_test)\n",
        "classifier = LogisticRegression(random_state=1693,solver='lbfgs')\n",
        "classifier.fit(X_train,Y_train)\n",
        "Y_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3FQUwCtKLdV"
      },
      "source": [
        "spc = ['Bad','Good']\n",
        "cm = confusion_matrix(Y_test,Y_pred)\n",
        "pd.DataFrame(cm, columns=spc, index=spc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}